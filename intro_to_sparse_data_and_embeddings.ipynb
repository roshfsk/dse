{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Sparse Data and Embeddings\n",
    "\n",
    "We're now going to look quickly at some textual data, to explore sparse data and work with embeddings.\n",
    "\n",
    "The text data is based on *movie review data*.  Our task is to predict if the review is generally *favorable* or *unfavorable*.\n",
    "\n",
    "This data has already been processed into tf.Example format.  Let's download the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!wget https://storage.googleapis.com/advanced-solutions-lab/mlcc/sparse_data_embedding/test.tfrecord -O /tmp/test.tfrecord\n",
    "!wget https://storage.googleapis.com/advanced-solutions-lab/mlcc/sparse_data_embedding/train.tfrecord -O /tmp/train.tfrecord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at one example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "record_iterator = tf.python_io.tf_record_iterator(path='/tmp/train.tfrecord')\n",
    "example = tf.train.Example()\n",
    "example.ParseFromString(record_iterator.next())\n",
    "print example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll turn these string-valued terms into feature vectors by using a vocabulary.\n",
    "\n",
    "The vocabulary is a list of each term we expect to see.  The first term listed will be mapped to the first coordinate in the feature vector, the second term to the second coordinate, and so on.\n",
    "\n",
    "For the purposes of this exercise, we've created a small vocabulary that focues on a limited set of terms.\n",
    "\n",
    "Most of these terms were found to be strongly indicative of *favorable* or *unfavorable*, but some were just added because they're interesting.\n",
    "\n",
    "Terms not appearing in this vocabulary are thrown away.\n",
    "\n",
    "We could of course use a larger vocabulary, and there are special tools for creating these.\n",
    "\n",
    "We could also use a *feature hashing* approach that hashes each term, instead of creating an explicit vocabulary.  This works well in practice, but loses interpretability which is useful for this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Use a linear model with sparse inputs and an explicit vocabulary\n",
    "\n",
    "First, we'll start with a linear model using these 36 informative terms -- always start simple!\n",
    "\n",
    "The `sparse_column_with_keys` feature column allows us to set up the string to feature vector mapping conveniently.\n",
    "\n",
    "After you read through the code, run it and see how we do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from IPython import display\n",
    "from sklearn import metrics\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# First, set up a dictionary that allows us to parse out the features from the\n",
    "# tf.Examples\n",
    "features_to_types_dict = {\n",
    "    \"terms\": tf.VarLenFeature(dtype=tf.string),\n",
    "    \"labels\": tf.FixedLenFeature(shape=[1], dtype=tf.float32)}\n",
    "\n",
    "# Create an input_fn that parses the tf.Examples from the given file pattern,\n",
    "# and split these into features and targets.\n",
    "def _input_fn(input_file_pattern):\n",
    "  features = tf.contrib.learn.io.read_batch_features(\n",
    "    file_pattern=input_file_pattern,\n",
    "    batch_size=25,\n",
    "    features=features_to_types_dict,\n",
    "    reader=tf.TFRecordReader)\n",
    "  targets = features.pop(\"labels\")\n",
    "  return features, targets\n",
    "\n",
    "informative_terms = [ \"bad\", \"great\", \"best\", \"worst\", \"fun\", \"beautiful\",\n",
    "                      \"excellent\", \"poor\", \"boring\", \"awful\", \"terrible\",\n",
    "                      \"definitely\", \"perfect\", \"liked\", \"worse\", \"waste\",\n",
    "                      \"entertaining\", \"loved\", \"unfortunately\", \"amazing\",\n",
    "                      \"enjoyed\", \"favorite\", \"horrible\", \"brilliant\", \"highly\",\n",
    "                      \"simple\", \"annoying\", \"today\", \"hilarious\", \"enjoyable\",\n",
    "                      \"dull\", \"fantastic\", \"poorly\", \"fails\", \"disappointing\",\n",
    "                      \"disappointment\", \"not\", \"him\", \"her\", \"good\", \"time\",\n",
    "                       \"?\", \".\", \"!\", \"movie\", \"film\", \"action\", \"comedy\",\n",
    "                       \"drama\", \"family\", \"man\", \"woman\", \"boy\", \"girl\" ]\n",
    "\n",
    "# Create a feature column from \"terms\", using our informative terms.\n",
    "terms_feature_column = tf.contrib.layers.sparse_column_with_keys(column_name=\"terms\",\n",
    "                                                                 keys=informative_terms)\n",
    "\n",
    "feature_columns = [ terms_feature_column ]\n",
    "\n",
    "classifier = tf.contrib.learn.LinearClassifier(\n",
    "  feature_columns=feature_columns,\n",
    "  optimizer=tf.train.AdagradOptimizer(\n",
    "    learning_rate=0.1),\n",
    "  gradient_clip_norm=5.0\n",
    ")\n",
    "\n",
    "classifier.fit(\n",
    "  input_fn=lambda: _input_fn(\"/tmp/train.tfrecord\"),\n",
    "  steps=1000)\n",
    "\n",
    "evaluation_metrics = classifier.evaluate(\n",
    "  input_fn=lambda: _input_fn(\"/tmp/train.tfrecord\"),\n",
    "  steps=1000)\n",
    "print \"Training set metrics:\"\n",
    "for m in evaluation_metrics:\n",
    "  print m, evaluation_metrics[m]\n",
    "print \"---\"\n",
    "\n",
    "evaluation_metrics = classifier.evaluate(\n",
    "  input_fn=lambda: _input_fn(\"/tmp/test.tfrecord\"),\n",
    "  steps=1000)\n",
    "\n",
    "print \"Test set metrics:\"\n",
    "for m in evaluation_metrics:\n",
    "  print m, evaluation_metrics[m]\n",
    "print \"---\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Use a deep model\n",
    "\n",
    "The above model is a linear model.  It works awfully well.  But can we do better with a Neural Net?  Let's try.\n",
    "\n",
    "Swap in a [DNNClassifier](https://www.tensorflow.org/api_docs/python/tf/contrib/learn/DNNClassifier) for the Linear Classifier and try to run it.  What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#@title Expand me for a possible solution\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from IPython import display\n",
    "from sklearn import metrics\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# First, set up a dictionary that allows us to parse out the features from the\n",
    "# tf.Examples\n",
    "features_to_types_dict = {\n",
    "    \"terms\": tf.VarLenFeature(dtype=tf.string),\n",
    "    \"labels\": tf.FixedLenFeature(shape=[1], dtype=tf.float32)}\n",
    "\n",
    "# Create an input_fn that parses the tf.Examples from the given file pattern,\n",
    "# and split these into features and targets.\n",
    "def _input_fn(input_file_pattern):\n",
    "  features = tf.contrib.learn.io.read_batch_features(\n",
    "    file_pattern=input_file_pattern,\n",
    "    batch_size=25,\n",
    "    features=features_to_types_dict,\n",
    "    reader=tf.TFRecordReader)\n",
    "  targets = features.pop(\"labels\")\n",
    "  return features, targets\n",
    "\n",
    "informative_terms = [ \"bad\", \"great\", \"best\", \"worst\", \"fun\", \"beautiful\",\n",
    "                      \"excellent\", \"poor\", \"boring\", \"awful\", \"terrible\",\n",
    "                      \"definitely\", \"perfect\", \"liked\", \"worse\", \"waste\",\n",
    "                      \"entertaining\", \"loved\", \"unfortunately\", \"amazing\",\n",
    "                      \"enjoyed\", \"favorite\", \"horrible\", \"brilliant\", \"highly\",\n",
    "                      \"simple\", \"annoying\", \"today\", \"hilarious\", \"enjoyable\",\n",
    "                      \"dull\", \"fantastic\", \"poorly\", \"fails\", \"disappointing\",\n",
    "                      \"disappointment\", \"not\", \"him\", \"her\", \"good\", \"time\",\n",
    "                       \"?\", \".\", \"!\", \"movie\", \"film\", \"action\", \"comedy\",\n",
    "                       \"drama\", \"family\", \"man\", \"woman\", \"boy\", \"girl\" ]\n",
    "\n",
    "# Create a feature column from \"terms\", using feature hashing.\n",
    "terms_feature_column = tf.contrib.layers.sparse_column_with_keys(column_name=\"terms\",\n",
    "                                                                 keys=informative_terms)\n",
    "\n",
    "feature_columns = [ terms_feature_column ]\n",
    "\n",
    "##################### Here's what we changed ##################################\n",
    "classifier = tf.contrib.learn.DNNClassifier(                                  #\n",
    "  feature_columns=feature_columns,                                            #\n",
    "  hidden_units=[20,20],                                                       #\n",
    "  optimizer=tf.train.AdagradOptimizer(                                        #\n",
    "    learning_rate=0.1),                                                       #\n",
    "  gradient_clip_norm=5.0                                                      #\n",
    ")                                                                             #\n",
    "###############################################################################\n",
    "\n",
    "classifier.fit(\n",
    "  input_fn=lambda: _input_fn(\"/tmp/train.tfrecord\"),\n",
    "  steps=1000)\n",
    "\n",
    "evaluation_metrics = classifier.evaluate(\n",
    "  input_fn=lambda: _input_fn(\"/tmp/train.tfrecord\"),\n",
    "  steps=1)\n",
    "print \"Training set metrics:\"\n",
    "for m in evaluation_metrics:\n",
    "  print m, evaluation_metrics[m]\n",
    "print \"---\"\n",
    "\n",
    "evaluation_metrics = classifier.evaluate(\n",
    "  input_fn=lambda: _input_fn(\"/tmp/test.tfrecord\"),\n",
    "  steps=1)\n",
    "\n",
    "print \"Test set metrics:\"\n",
    "for m in evaluation_metrics:\n",
    "  print m, evaluation_metrics[m]\n",
    "print \"---\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Use an embedding with a deep model.\n",
    "\n",
    "Ah, right, we need to use an embedding, which serves as a nice adapter for sparse data to be used as input to a deep Neural Net.\n",
    "\n",
    "This is easy to set up using an `embedding_column`, which is a feature column that take sparse data as input and gives a lower-dimensional dense vector as an output, using an embedding to do the conversion.\n",
    "\n",
    "Go ahead insert an embedding_column, projecting into 2 dimensions.  Here's a example code snippet you might use:\n",
    "\n",
    "`terms_embedding_column = tf.contrib.layers.embedding_column(terms_feature_column, dimension=2)`\n",
    "\n",
    "`feature_columns = [ terms_embedding_column ]`\n",
    "\n",
    "In practice, we might project to dimensions higher than 2, like 50 or 100.  But for now, 2 dimensions is easy to visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#@title Expand for a possible solution\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from IPython import display\n",
    "from sklearn import metrics\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# First, set up a dictionary that allows us to parse out the features from the\n",
    "# tf.Examples\n",
    "features_to_types_dict = {\n",
    "    \"terms\": tf.VarLenFeature(dtype=tf.string),\n",
    "    \"labels\": tf.FixedLenFeature(shape=[1], dtype=tf.float32)}\n",
    "\n",
    "# Create an input_fn that parses the tf.Examples from the given file pattern,\n",
    "# and split these into features and targets.\n",
    "def _input_fn(input_file_pattern):\n",
    "  features = tf.contrib.learn.io.read_batch_features(\n",
    "    file_pattern=input_file_pattern,\n",
    "    batch_size=25,\n",
    "    features=features_to_types_dict,\n",
    "    reader=tf.TFRecordReader)\n",
    "  targets = features.pop(\"labels\")\n",
    "  return features, targets\n",
    "\n",
    "\n",
    "informative_terms = [ \"bad\", \"great\", \"best\", \"worst\", \"fun\", \"beautiful\",\n",
    "                      \"excellent\", \"poor\", \"boring\", \"awful\", \"terrible\",\n",
    "                      \"definitely\", \"perfect\", \"liked\", \"worse\", \"waste\",\n",
    "                      \"entertaining\", \"loved\", \"unfortunately\", \"amazing\",\n",
    "                      \"enjoyed\", \"favorite\", \"horrible\", \"brilliant\", \"highly\",\n",
    "                      \"simple\", \"annoying\", \"today\", \"hilarious\", \"enjoyable\",\n",
    "                      \"dull\", \"fantastic\", \"poorly\", \"fails\", \"disappointing\",\n",
    "                      \"disappointment\", \"not\", \"him\", \"her\", \"good\", \"time\",\n",
    "                       \"?\", \".\", \"!\", \"movie\", \"film\", \"action\", \"comedy\",\n",
    "                       \"drama\", \"family\", \"man\", \"woman\", \"boy\", \"girl\" ]\n",
    "\n",
    "# Create a feature column from \"terms\", using feature hashing.\n",
    "terms_feature_column = tf.contrib.layers.sparse_column_with_keys(column_name=\"terms\",\n",
    "                                                                 keys=informative_terms)\n",
    "\n",
    "############################# Here's what we changed ###########################################\n",
    "terms_embedding_column = tf.contrib.layers.embedding_column(terms_feature_column, dimension=2) #\n",
    "feature_columns = [ terms_embedding_column ]                                                   #\n",
    "################################################################################################\n",
    "\n",
    "classifier = tf.contrib.learn.DNNClassifier(\n",
    "  feature_columns=feature_columns,\n",
    "  hidden_units=[10,10],\n",
    "  optimizer=tf.train.AdagradOptimizer(\n",
    "    learning_rate=0.1),\n",
    "  gradient_clip_norm=5.0\n",
    ")\n",
    "\n",
    "classifier.fit(\n",
    "  input_fn=lambda: _input_fn(\"/tmp/train.tfrecord\"),\n",
    "  steps=1000)\n",
    "\n",
    "evaluation_metrics = classifier.evaluate(\n",
    "  input_fn=lambda: _input_fn(\"/tmp/train.tfrecord\"),\n",
    "  steps=1000)\n",
    "print \"Training set metrics:\"\n",
    "for m in evaluation_metrics:\n",
    "  print m, evaluation_metrics[m]\n",
    "print \"---\"\n",
    "\n",
    "evaluation_metrics = classifier.evaluate(\n",
    "  input_fn=lambda: _input_fn(\"/tmp/test.tfrecord\"),\n",
    "  steps=1000)\n",
    "\n",
    "print \"Test set metrics:\"\n",
    "for m in evaluation_metrics:\n",
    "  print m, evaluation_metrics[m]\n",
    "print \"---\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Convince yourself there's actually an embedding in there\n",
    "\n",
    "The above model used something called an `embedding_column`, and it seemed to work but this doesn't tell us much about what's going on internally.\n",
    "\n",
    "How do we know this was actually using an embedding inside?  How would we check?\n",
    "\n",
    "To start, let's look at the tensors in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier.get_variable_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier.get_variable_value('dnn/input_from_feature_columns/terms_embedding/weights').shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's something interesting here, by the way: adding an `embedding_column` created a new layer in the model, and this layer is trainable along with the rest of the model just as any hidden layer is.\n",
    "\n",
    "Spend some time manually checking the various layers and shapes to make sure everything is connected the way you would expect it would be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5: Examine the Embedding\n",
    "\n",
    "Okay, let's now take a look at the actual space.  Run the following to see where the terms end up in the embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "embedding_matrix = classifier.get_variable_value('dnn/input_from_feature_columns/terms_embedding/weights')\n",
    "\n",
    "for term_index in range(len(informative_terms)):\n",
    "  # Create a one-hot encoding for our term.  It has 0's everywhere, except for\n",
    "  # a single 1 in the coordinate that corresponds to that term.\n",
    "  term_vector = np.zeros(len(informative_terms))\n",
    "  term_vector[term_index] = 1\n",
    "  # We'll now project that one-hot vector into the embedding space.\n",
    "  embedding_xy = np.matmul(term_vector, embedding_matrix)\n",
    "  plt.text(embedding_xy[0],\n",
    "           embedding_xy[1],\n",
    "           informative_terms[term_index])\n",
    "\n",
    "# Do a little set-up to make sure the plot displays nicely.\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 12)\n",
    "plt.xlim(1.2 * embedding_matrix.min(), 1.2 * embedding_matrix.max())\n",
    "plt.ylim(1.2 * embedding_matrix.min(), 1.2 * embedding_matrix.max())\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this once with an embedding from a trained model.  Do things end up where you'd expect?\n",
    "\n",
    "Re-train the model and run the embedding visualization again.  What stays the same?  What changes?\n",
    "\n",
    "Finally, re-train the model again with just a single step.  (This will yield a terrible model.)  Run the embedding visualization again.  What do you see now, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6:  Try to Improve the Model's Performance\n",
    "\n",
    "Go ahead and try to change the model to improve performance.\n",
    "\n",
    "This can be done by changing hyperparameters, or using a different optimizer like Adam or Adagrad.\n",
    "\n",
    "But you might only get one or two percentage points following these strategies.\n",
    "\n",
    "You might also get some wins by adding additional terms.\n",
    "\n",
    "There's a full vocabulary file with all 30716 terms for this data set that you can use at: `https://storage.googleapis.com/advanced-solutions-lab/mlcc/sparse_data_embedding/terms.txt`\n",
    "\n",
    "You can pick out additional terms from this vocabulary file, or use the whole thing via the `sparse_column_with_vocabulary_file` feature column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!wget https://storage.googleapis.com/advanced-solutions-lab/mlcc/sparse_data_embedding/terms.txt -O /tmp/terms.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from IPython import display\n",
    "from sklearn import metrics\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# First, set up a dictionary that allows us to parse out the features from the\n",
    "# tf.Examples\n",
    "features_to_types_dict = {\n",
    "    \"terms\": tf.VarLenFeature(dtype=tf.string),\n",
    "    \"labels\": tf.FixedLenFeature(shape=[1], dtype=tf.float32)}\n",
    "\n",
    "# Create an input_fn that parses the tf.Examples from the given file pattern,\n",
    "# and split these into features and targets.\n",
    "def _input_fn(input_file_pattern):\n",
    "  features = tf.contrib.learn.io.read_batch_features(\n",
    "    file_pattern=input_file_pattern,\n",
    "    batch_size=100,\n",
    "    features=features_to_types_dict,\n",
    "    reader=tf.TFRecordReader)\n",
    "  targets = features.pop(\"labels\")\n",
    "  return features, targets\n",
    "\n",
    "# Create a feature column from \"terms\", using a full vocabulary file.\n",
    "informative_terms = None\n",
    "with open(\"/tmp/terms.txt\", 'r') as f:\n",
    "  # Convert it to set first to remove duplicates.\n",
    "  informative_terms = list(set(f.read().split()))\n",
    "terms_feature_column = tf.contrib.layers.sparse_column_with_keys(column_name=\"terms\",\n",
    "                                                                 keys=informative_terms)\n",
    "\n",
    "terms_embedding_column = tf.contrib.layers.embedding_column(terms_feature_column, dimension=2)\n",
    "feature_columns = [ terms_embedding_column ]\n",
    "\n",
    "classifier = tf.contrib.learn.DNNClassifier(\n",
    "  feature_columns=feature_columns,\n",
    "  hidden_units=[10, 10],\n",
    "  optimizer=tf.train.AdamOptimizer(\n",
    "    learning_rate=0.001),\n",
    "  gradient_clip_norm=1.0\n",
    ")\n",
    "\n",
    "classifier.fit(\n",
    "  input_fn=lambda: _input_fn(\"/tmp/train.tfrecord\"),\n",
    "  steps=1000)\n",
    "\n",
    "evaluation_metrics = classifier.evaluate(\n",
    "  input_fn=lambda: _input_fn(\"/tmp/train.tfrecord\"),\n",
    "  steps=1000)\n",
    "print \"Training set metrics:\"\n",
    "for m in evaluation_metrics:\n",
    "  print m, evaluation_metrics[m]\n",
    "print \"---\"\n",
    "\n",
    "evaluation_metrics = classifier.evaluate(\n",
    "  input_fn=lambda: _input_fn(\"/tmp/test.tfrecord\"),\n",
    "  steps=1000)\n",
    "\n",
    "print \"Test set metrics:\"\n",
    "for m in evaluation_metrics:\n",
    "  print m, evaluation_metrics[m]\n",
    "print \"---\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A final word\n",
    "One final thing to note here. We may have gotten a DNN solution with an embedding that was better than our original linear model, but the linear model was also pretty good and was quite a bit faster to train.\n",
    "The faster training is because linear models do not have nearly as many parameters to update or layers to back-prop through.\n",
    "In some applications, the speed of linear models may be a game changer, or linear models may be perfectly sufficient from a quality standpoint.\n",
    "In others areas, the additional model complexity and capacity provided by DNN's might be more important.\n",
    "The key is to remember to explore your problem sufficiently so that you know which space you're in."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
