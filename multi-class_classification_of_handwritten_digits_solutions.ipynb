{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Handwritten Digits with Neural Networks - Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!wget https://storage.googleapis.com/advanced-solutions-lab/mnist/mnist_train_small.csv -O /tmp/mnist_train_small.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from IPython import display\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = '{:.1f}'.format\n",
    "\n",
    "mnist_dataframe = pd.read_csv(\n",
    "  io.open(\"/tmp/mnist_train_small.csv\", \"r\"),\n",
    "  sep=\",\",\n",
    "  header=None)\n",
    "\n",
    "mnist_dataframe = mnist_dataframe.reindex(np.random.permutation(mnist_dataframe.index))\n",
    "mnist_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_labels_and_features(dataset):\n",
    "  \"\"\"Extracts labels and features.\n",
    "  \n",
    "  This is a good place to scale or transform the features if needed.\n",
    "  \n",
    "  Args:\n",
    "    dataset: A Pandas `Dataframe`, containing the label on the first column and\n",
    "      monochrome pixel values on the remaining columns, in row major order.\n",
    "  Returns:\n",
    "    A `tuple` `(labels, features)`:\n",
    "      labels: A Pandas `Series`.\n",
    "      features: A Pandas `DataFrame`.\n",
    "  \"\"\"\n",
    "  labels = dataset[0]\n",
    "\n",
    "  # DataFrame.loc index ranges are inclusive at both ends.\n",
    "  features = dataset.loc[:,1:784]\n",
    "  # Scale the data to [0, 1] by dividing out the max value, 255.\n",
    "  features = features / 255\n",
    "\n",
    "  return labels, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_targets, training_examples = parse_labels_and_features(mnist_dataframe.head(15000))\n",
    "training_examples.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "validation_targets, validation_examples = parse_labels_and_features(mnist_dataframe.tail(5000))\n",
    "validation_examples.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_linear_classification_model(\n",
    "    learning_rate,\n",
    "    steps,\n",
    "    batch_size,\n",
    "    training_examples,\n",
    "    training_targets,\n",
    "    validation_examples,\n",
    "    validation_targets):\n",
    "  \"\"\"Trains a linear classification model for the MNIST digits dataset.\n",
    "  \n",
    "  In addition to training, this function also prints training progress information,\n",
    "  a plot of the training and validation loss over time, and a confusion\n",
    "  matrix.\n",
    "  \n",
    "  Args:\n",
    "    learning_rate: An `int`, the learning rate to use.\n",
    "    steps: A non-zero `int`, the total number of training steps. A training step\n",
    "      consists of a forward and backward pass using a single batch.\n",
    "    batch_size: A non-zero `int`, the batch size.\n",
    "    training_examples: A `DataFrame` containing the training features.\n",
    "    training_targets: A `DataFrame` containing the training labels.\n",
    "    validation_examples: A `DataFrame` containing the validation features.\n",
    "    validation_targets: A `DataFrame` containing the validation labels.\n",
    "      \n",
    "  Returns:\n",
    "    The trained `LinearClassifier` object.\n",
    "  \"\"\"\n",
    "\n",
    "  periods = 10\n",
    "  steps_per_period = steps / periods\n",
    "\n",
    "  # Create a linear classifier object.\n",
    "  feature_columns = tf.contrib.learn.infer_real_valued_columns_from_input(\n",
    "      training_examples)\n",
    "  classifier = tf.contrib.learn.LinearClassifier(\n",
    "      feature_columns=feature_columns,\n",
    "      n_classes=10,\n",
    "      optimizer=tf.train.AdagradOptimizer(learning_rate=learning_rate),\n",
    "      gradient_clip_norm=5.0\n",
    "  )\n",
    "\n",
    "  # Train the model, but do so inside a loop so that we can periodically assess\n",
    "  # loss metrics.\n",
    "  print \"Training model...\"\n",
    "  print \"LogLoss error (on validation data):\"\n",
    "  training_errors = []\n",
    "  validation_errors = []\n",
    "  for period in range (0, periods):\n",
    "    # Train the model, starting from the prior state.\n",
    "    classifier.fit(\n",
    "        training_examples,\n",
    "        training_targets,\n",
    "        steps=steps_per_period,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    # Take a break and compute predictions.\n",
    "    training_predictions = list(classifier.predict_proba(training_examples))\n",
    "    validation_predictions = list(classifier.predict_proba(validation_examples))\n",
    "    # Compute training and validation errors.\n",
    "    training_log_loss = metrics.log_loss(training_targets, training_predictions)\n",
    "    validation_log_loss = metrics.log_loss(validation_targets, validation_predictions)\n",
    "    # Occasionally print the current loss.\n",
    "    print \"  period %02d : %0.2f\" % (period, validation_log_loss)\n",
    "    # Add the loss metrics from this period to our list.\n",
    "    training_errors.append(training_log_loss)\n",
    "    validation_errors.append(validation_log_loss)\n",
    "  print \"Model training finished.\"\n",
    "\n",
    "  # Calculate final predictions (not probabilities, as above).\n",
    "  final_predictions = list(classifier.predict(validation_examples))\n",
    "  accuracy = metrics.accuracy_score(validation_targets, final_predictions)\n",
    "  print \"Final accuracy (on validation data): %0.2f\" % accuracy  \n",
    "\n",
    "  # Output a graph of loss metrics over periods.\n",
    "  plt.ylabel(\"LogLoss\")\n",
    "  plt.xlabel(\"Periods\")\n",
    "  plt.title(\"LogLoss vs. Periods\")\n",
    "  plt.plot(training_errors, label=\"training\")\n",
    "  plt.plot(validation_errors, label=\"validation\")\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "  \n",
    "  # Output a plot of the confusion matrix.\n",
    "  cm = confusion_matrix(validation_targets, final_predictions)\n",
    "  # Normalize the confusion matrix by row (i.e by the number of samples\n",
    "  # in each class)\n",
    "  cm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "  ax = sns.heatmap(cm_normalized, cmap=\"bone_r\")\n",
    "  ax.set_aspect(1)\n",
    "  plt.title(\"Confusion matrix\")\n",
    "  plt.ylabel(\"True label\")\n",
    "  plt.xlabel(\"Predicted label\")\n",
    "  plt.show()\n",
    "\n",
    "  return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution to Task 1\n",
    "\n",
    "Here is a set of parameters that should attain roughly 0.9 accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = train_linear_classification_model(\n",
    "    learning_rate=0.03,\n",
    "    steps=1000,\n",
    "    batch_size=100,\n",
    "    training_examples=training_examples,\n",
    "    training_targets=training_targets,\n",
    "    validation_examples=validation_examples,\n",
    "    validation_targets=validation_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution to Task 2\n",
    "\n",
    "The code below is an almost identical copy of the original training code, with the exception of the neural network specific code, like the hyperparameter for hidden units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_nn_classification_model(\n",
    "    learning_rate,\n",
    "    steps,\n",
    "    batch_size,\n",
    "    hidden_units,\n",
    "    training_examples,\n",
    "    training_targets,\n",
    "    validation_examples,\n",
    "    validation_targets):\n",
    "  \"\"\"Trains a neural network classification model for the MNIST digits dataset.\n",
    "  \n",
    "  In addition to training, this function also prints training progress information,\n",
    "  a plot of the training and validation loss over time, as well as a confusion\n",
    "  matrix.\n",
    "  \n",
    "  Args:\n",
    "    learning_rate: An `int`, the learning rate to use.\n",
    "    steps: A non-zero `int`, the total number of training steps. A training step\n",
    "      consists of a forward and backward pass using a single batch.\n",
    "    batch_size: A non-zero `int`, the batch size.\n",
    "    hidden_units: A `list` of int values, specifying the number of neurons in each layer.\n",
    "    training_examples: A `DataFrame` containing the training features.\n",
    "    training_targets: A `DataFrame` containing the training labels.\n",
    "    validation_examples: A `DataFrame` containing the validation features.\n",
    "    validation_targets: A `DataFrame` containing the validation labels.\n",
    "      \n",
    "  Returns:\n",
    "    The trained `DNNClassifier` object.\n",
    "  \"\"\"\n",
    "\n",
    "  periods = 10\n",
    "  steps_per_period = steps / periods\n",
    "\n",
    "  # Create a linear classifier object.\n",
    "  feature_columns = tf.contrib.learn.infer_real_valued_columns_from_input(\n",
    "      training_examples)\n",
    "  classifier = tf.contrib.learn.DNNClassifier(\n",
    "      feature_columns=feature_columns,\n",
    "      n_classes=10,\n",
    "      hidden_units=hidden_units,\n",
    "      optimizer=tf.train.AdagradOptimizer(learning_rate=learning_rate),\n",
    "      gradient_clip_norm=5.0\n",
    "  )\n",
    "\n",
    "  # Train the model, but do so inside a loop so that we can periodically assess\n",
    "  # loss metrics.\n",
    "  print \"Training model...\"\n",
    "  print \"LogLoss error (on validation data):\"\n",
    "  training_errors = []\n",
    "  validation_errors = []\n",
    "  for period in range (0, periods):\n",
    "    # Train the model, starting from the prior state.\n",
    "    classifier.fit(\n",
    "        training_examples,\n",
    "        training_targets,\n",
    "        steps=steps_per_period,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    # Take a break and compute predictions.\n",
    "    training_predictions = list(classifier.predict_proba(training_examples))\n",
    "    validation_predictions = list(classifier.predict_proba(validation_examples))\n",
    "    # Compute training and validation errors.\n",
    "    training_log_loss = metrics.log_loss(training_targets, training_predictions)\n",
    "    validation_log_loss = metrics.log_loss(validation_targets, validation_predictions)\n",
    "    # Occasionally print the current loss.\n",
    "    print \"  period %02d : %0.2f\" % (period, validation_log_loss)\n",
    "    # Add the loss metrics from this period to our list.\n",
    "    training_errors.append(training_log_loss)\n",
    "    validation_errors.append(validation_log_loss)\n",
    "  print \"Model training finished.\"\n",
    "\n",
    "  # Calculate final predictions (not probabilities, as above).\n",
    "  final_predictions = list(classifier.predict(validation_examples))\n",
    "  accuracy = metrics.accuracy_score(validation_targets, final_predictions)\n",
    "  print \"Final accuracy (on validation data): %0.2f\" % accuracy  \n",
    "\n",
    "  # Output a graph of loss metrics over periods.\n",
    "  plt.ylabel(\"LogLoss\")\n",
    "  plt.xlabel(\"Periods\")\n",
    "  plt.title(\"LogLoss vs. Periods\")\n",
    "  plt.plot(training_errors, label=\"training\")\n",
    "  plt.plot(validation_errors, label=\"validation\")\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "  \n",
    "  # Output a plot of the confusion matrix.\n",
    "  cm = confusion_matrix(validation_targets, final_predictions)\n",
    "  # Normalize the confusion matrix by row (i.e by the number of samples\n",
    "  # in each class)\n",
    "  cm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "  ax = sns.heatmap(cm_normalized, cmap=\"bone_r\")\n",
    "  ax.set_aspect(1)\n",
    "  plt.title(\"Confusion matrix\")\n",
    "  plt.ylabel(\"True label\")\n",
    "  plt.xlabel(\"Predicted label\")\n",
    "  plt.show()\n",
    "\n",
    "  return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = train_nn_classification_model(\n",
    "    learning_rate=0.05,\n",
    "    steps=1000,\n",
    "    batch_size=100,\n",
    "    hidden_units=[100, 100],\n",
    "    training_examples=training_examples,\n",
    "    training_targets=training_targets,\n",
    "    validation_examples=validation_examples,\n",
    "    validation_targets=validation_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we verify the accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!wget https://storage.googleapis.com/advanced-solutions-lab/mnist/mnist_test.csv -O /tmp/mnist_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mnist_test_dataframe = pd.read_csv(\n",
    "  io.open(\"/tmp/mnist_test.csv\", \"r\"),\n",
    "  sep=\",\",\n",
    "  header=None)\n",
    "\n",
    "test_targets, test_examples = parse_labels_and_features(mnist_test_dataframe)\n",
    "test_examples.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_predictions = list(classifier.predict(test_examples))\n",
    "accuracy = metrics.accuracy_score(test_targets, test_predictions)\n",
    "print \"Accuracy on test data: %0.2f\" % accuracy  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
